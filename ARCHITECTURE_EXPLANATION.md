# Focus Guard - Complete Architecture Explanation

## ğŸ“ Project Structure

```
Project-Phone-Detector/
â”œâ”€â”€ index.html          # UI structure (3 screens)
â”œâ”€â”€ detection-engine.js # All logic & AI processing
â”œâ”€â”€ monitor-ui.css      # Styling
â”œâ”€â”€ yolov5n.onnx       # AI model file
â””â”€â”€ package.json       # Node.js server config
```

---

## ğŸ¯ **How Everything Works Together**

### **1. INITIALIZATION FLOW**

```
Page Loads
    â†“
index.html loads detection-engine.js
    â†“
detection-engine.js runs:
    - Gets all HTML elements (video, canvas, buttons)
    - Shows homepage screen
    - Initializes audio system
    â†“
User sees homepage with toggle & start button
```

### **2. USER INTERACTION FLOW**

```
Homepage
    â†“
User toggles "Enable Phone Detection" ON
    â†“
"Start Monitoring" button becomes enabled
    â†“
User clicks "Start Monitoring"
    â†“
Permission Screen appears
    â†“
User clicks "Grant Camera Access"
    â†“
Browser requests camera permission
    â†“
If granted:
    - Video stream starts
    - Detection screen appears
    - startSystem() is called
```

### **3. SYSTEM STARTUP (startSystem function)**

```
startSystem() called
    â†“
Step 1: Load ONNX.js library
    - Creates <script> tag
    - Loads from CDN
    - Waits for window.ort to be available
    â†“
Step 2: Initialize ONNX Runtime
    - Sets WASM to single thread
    - Disables SIMD
    â†“
Step 3: Load YOLOv5 Model
    - Reads yolov5n.onnx file
    - Creates InferenceSession
    - Model ready for predictions
    â†“
Step 4: Wait for video stream
    - Checks every 100ms if video is ready
    - When ready, starts detection loop
```

### **4. DETECTION LOOP (predictWebcam function)**

This runs every 2 seconds:

```
predictWebcam() called
    â†“
Check 1: Is system running? â†’ No? Exit
Check 2: Is video ready? â†’ No? Retry in 500ms
Check 3: Has 2 seconds passed? â†’ No? Retry in 500ms
Check 4: Is already processing? â†’ Yes? Retry in 500ms
    â†“
All checks pass â†’ Start processing
    â†“
Yield to browser (setTimeout 200ms)
    â†“
PREPROCESSING:
    - Create temporary canvas
    - Draw video frame to canvas (resize to 640x640)
    - Get pixel data
    - Convert RGB to BGR
    - Normalize values (0-255 â†’ 0-1)
    - Create Float32Array tensor
    â†“
Yield to browser (10ms)
    â†“
CREATE TENSOR:
    - Shape: [1, 3, 640, 640]
    - 1 = batch size
    - 3 = RGB channels
    - 640x640 = image size
    â†“
Yield to browser (10ms)
    â†“
RUN AI MODEL:
    - model.run(feeds)
    - This is the BLOCKING operation
    - Takes 100-500ms
    - Returns detection results
    â†“
POSTPROCESSING:
    - Parse output array
    - Extract bounding boxes
    - Filter by confidence (15% threshold)
    - Filter by class (only class 67 = cell phone)
    - Apply Non-Maximum Suppression (remove duplicates)
    - Scale coordinates to video size
    â†“
Yield to browser (10ms)
    â†“
DISPLAY RESULTS:
    - Clear canvas
    - Draw red boxes around phones
    - Update status panel
    - Play sound if phone found
    - Show notification if tab inactive
    â†“
Schedule next detection (1000ms delay)
```

### **5. DATA FLOW**

```
Video Stream (Webcam)
    â†“
<video> element displays live feed
    â†“
predictWebcam() captures frame
    â†“
preprocess() converts to tensor
    â†“
YOLOv5 Model processes tensor
    â†“
Model outputs: [1, 25200, 85]
    - 25200 = possible detections
    - 85 = [x, y, w, h, conf, 80 class scores]
    â†“
postprocess() filters results
    â†“
Only phones (class 67) with >15% confidence
    â†“
displayDetections() draws boxes
    â†“
<canvas> overlay shows boxes
    â†“
updateStatus() updates UI
```

---

## ğŸ”— **How Components Connect**

### **HTML â†’ JavaScript**

```javascript
// HTML defines elements:
<button id="start-btn-alt">Start Monitoring</button>
<video id="webcam"></video>
<canvas id="canvas-overlay"></canvas>

// JavaScript gets references:
const startBtnAlt = document.getElementById('start-btn-alt');
const video = document.getElementById('webcam');
const canvas = document.getElementById('canvas-overlay');
```

### **Event Listeners Chain**

```
Toggle Change Event
    â†“
syncToggles() updates both toggles
    â†“
Enables/disables start buttons
    â†“
Start Button Click
    â†“
showScreen('permission')
    â†“
Permission Button Click
    â†“
getUserMedia() requests camera
    â†“
startSystem() loads AI model
    â†“
predictWebcam() starts detection loop
```

### **State Management**

```javascript
// Global state variables:
let model = null;              // AI model instance
let isRunning = false;         // Detection active?
let isProcessing = false;      // Currently processing?
let lastPredictionTime = 0;    // Rate limiting
let soundEnabled = true;       // Sound alerts on/off
```

---

## ğŸ¨ **UI Screen System**

### **Three Screens (only one visible at a time)**

1. **Homepage** (`#homepage`)
   - Marketing content
   - Toggle & start button
   - Features section

2. **Permission Screen** (`#permission-screen`)
   - Camera permission request
   - Privacy notice

3. **Detection Screen** (`#detection-screen`)
   - Live video feed
   - Canvas overlay (boxes)
   - Status panel
   - Stop button

**Switching screens:**
```javascript
showScreen('homepage')    // Shows homepage, hides others
showScreen('permission')  // Shows permission, hides others
showScreen('detection')   // Shows detection, hides others
```

---

## ğŸ¤– **AI Detection Process**

### **YOLOv5 Model**

- **Input**: 640x640 RGB image (normalized 0-1)
- **Output**: Array of detections
- **Format**: [batch, num_boxes, 85]
  - Each box has: x, y, width, height, confidence, 80 class scores
- **Class 67** = "cell phone" in COCO dataset

### **Detection Pipeline**

```
Raw Video Frame
    â†“
Resize to 640x640
    â†“
RGB â†’ BGR conversion
    â†“
Normalize (0-1)
    â†“
Tensor [1, 3, 640, 640]
    â†“
YOLOv5 Inference
    â†“
Raw Output [1, 25200, 85]
    â†“
Filter by confidence (>10%)
    â†“
Find best class for each box
    â†“
Filter by class (only 67)
    â†“
Filter by final score (>15%)
    â†“
Non-Maximum Suppression
    â†“
Scale to video dimensions
    â†“
Draw boxes on canvas
```

---

## ğŸ”Š **Sound & Notification System**

### **Sound Alert**

```javascript
playAlertSound()
    â†“
Checks: soundEnabled? audioContext exists?
    â†“
Checks cooldown (2 seconds)
    â†“
Creates Web Audio oscillator
    â†“
Plays 800Hz beep for 0.5 seconds
    â†“
Works even when tab is inactive!
```

### **Browser Notification**

```javascript
updateStatus() detects phone
    â†“
Checks: tab hidden? permission granted?
    â†“
Creates browser notification
    â†“
Shows even when tab is inactive
```

---

## âš¡ **Performance Optimizations**

### **Non-Blocking Design**

1. **Rate Limiting**: Only runs every 2 seconds
2. **Yielding**: Uses setTimeout between steps
3. **requestIdleCallback**: Runs when browser is idle
4. **Single Thread**: WASM uses 1 thread to prevent blocking
5. **Overlap Prevention**: `isProcessing` flag prevents concurrent runs

### **Why This Matters**

- Without these: Page freezes, can't click anything
- With these: Page stays responsive, smooth experience

---

## ğŸ”„ **Complete User Journey**

```
1. User opens website
   â†’ Homepage loads
   â†’ JavaScript initializes

2. User reads content
   â†’ Sees hero, about, features

3. User enables toggle
   â†’ Toggle syncs
   â†’ Start button enables

4. User clicks "Start Monitoring"
   â†’ Permission screen shows
   â†’ User grants camera access

5. System starts
   â†’ Loads ONNX.js (from CDN)
   â†’ Loads YOLOv5 model (from file)
   â†’ Starts video stream

6. Detection begins
   â†’ Every 2 seconds:
     - Captures frame
     - Preprocesses
     - Runs AI
     - Postprocesses
     - Displays results

7. Phone detected
   â†’ Red box drawn
   â†’ Sound plays
   â†’ Status updates
   â†’ Notification shows (if tab inactive)

8. User clicks "Stop"
   â†’ Stops video stream
   â†’ Returns to homepage
   â†’ Resets toggles
```

---

## ğŸ› ï¸ **Key Technologies**

- **ONNX.js**: Runs AI models in browser
- **YOLOv5 Nano**: Lightweight object detection model
- **WebRTC**: Camera access (getUserMedia)
- **Canvas API**: Drawing detection boxes
- **Web Audio API**: Sound alerts
- **Notifications API**: Browser notifications

---

## ğŸ“Š **Data Structures**

### **Detection Object**
```javascript
{
    x: 100,           // Top-left X coordinate
    y: 200,           // Top-left Y coordinate
    width: 150,       // Box width
    height: 200,      // Box height
    class: 67,        // COCO class ID
    className: "cell phone",
    score: 0.85       // Confidence (0-1)
}
```

### **Model Input**
```javascript
Tensor {
    type: 'float32',
    data: Float32Array[1228800],  // 3 * 640 * 640
    dims: [1, 3, 640, 640]
}
```

---

## ğŸ¯ **Summary**

**Everything connects through:**
1. **HTML** provides structure and elements
2. **JavaScript** gets elements and adds event listeners
3. **Events** trigger functions (clicks, toggles)
4. **Functions** call other functions in sequence
5. **AI Model** processes video frames
6. **Results** update UI and trigger alerts
7. **Loop** continues every 2 seconds

**The magic happens in `predictWebcam()` - it's the heart of the system!**

